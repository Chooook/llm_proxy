import json
import os
from typing import Awaitable, Callable, Dict

from aioredis import Redis
from loguru import logger

from settings import settings
from utils.redis_utils import mark_task_failed


def register_handlers(
        ) -> Dict[str, Callable[[str, Redis], Awaitable[None]]]:
    """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∑–∞–¥–∞—á"""
    task_handlers: Dict[str, Callable[[str, Redis], Awaitable[None]]] = {}
    try:
        import llama_cpp
        if not os.path.exists(settings.MODEL_PATH):
            raise FileNotFoundError
        task_handlers['generate_local'] = _handle_generate_local_task
        logger.info("‚úÖ LLM –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω")
    except ImportError:
        logger.warning(
            "‚ö†Ô∏è LLM –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    except FileNotFoundError:
        logger.warning(
            "‚ö†Ô∏è LLM –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: "
            f"{settings.MODEL_PATH}")
    return task_handlers


async def _handle_generate_local_task(task_id: str, redis: Redis):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –∑–∞–¥–∞—á —Å –ª–æ–∫–∞–ª—å–Ω—ã–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º"""
    try:
        from llama_cpp import (Llama,
                               ChatCompletionRequestUserMessage,
                               ChatCompletionRequestSystemMessage)
    except ImportError:
        await mark_task_failed(
            redis,
            task_id,
            "LLM –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏"
        )
        raise RuntimeError("LLM –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

    task_data = await redis.get(f'task:{task_id}')
    if not task_data:
        logger.warning(f'‚ö†Ô∏è –ó–∞–¥–∞—á–∞ {task_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞')
        return

    # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    if not hasattr(_handle_generate_local_task, 'llm'):
        try:
            _handle_generate_local_task.llm = Llama(
                model_path=settings.MODEL_PATH,
                n_ctx=65536,
                n_thread=12,
                n_batch=512,
                verbose=False
            )
            logger.info('‚úÖ –ú–æ–¥–µ–ª—å LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞')
        except Exception as e:
            await mark_task_failed(
                redis,
                task_id,
                f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏: {str(e)}"
            )
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –º–æ–¥–µ–ª–∏: {e}")

    task = json.loads(task_data)
    prompt = task['prompt']
    logger.debug(f'üß† –ü–æ–ª—É—á–µ–Ω prompt: {prompt}')
    try:
        system_message: ChatCompletionRequestSystemMessage = {
            "role": "system",
            "content": "–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π –¥–∞—ë—Ç –∫—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã.",
        }
        user_message = ChatCompletionRequestUserMessage(
            role="user",
            content=prompt
        )
        output = _handle_generate_local_task.llm.create_chat_completion(
            messages=[system_message, user_message],
            max_tokens=512,
        )
        result = output['choices'][0]['message']['content'].strip()
        logger.debug(f'üß† –†–µ–∑—É–ª—å—Ç–∞—Ç: {result}')

        task['status'] = 'completed'
        task['result'] = result
        await redis.setex(f'task:{task_id}', 86400, json.dumps(task))
        logger.info(f'‚úÖ –ó–∞–¥–∞—á–∞ {task_id} –≤—ã–ø–æ–ª–Ω–µ–Ω–∞')

    except Exception as e:
        await mark_task_failed(
            redis,
            task_id,
            f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ LLM: {str(e)}"
        )
        raise


async def _handle_generate_api_task(task_id: str, redis: Redis):
    # TODO
    pass
